[BASICS]
device_map = auto
dataset = imdb
model = mistralai/Mistral-7B-Instruct-v0.3

[LORA]
alpha = 8
dropout = 0.1
r = 32
bias = none
task = CASUAL_LM

[TRAINING]
output_dir = ./results/models
optim = adamw_torch
bf16 = True 

per_device_train_batch_size = 8
gradient_accumulation_steps = 2

save_steps = 10
logging_steps = 1

max_grad_norm = 0.3
warmup_ratio = 0.1

max_seq_length = 512
max_steps = 3

lr_scheduler_type = cosine
learning_rate = 2e-4

reentrant = False