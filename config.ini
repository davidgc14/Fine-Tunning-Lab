[BASICS]
device_map = 
dataset = timdettmers/openassistant-guanaco
model = TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T

[LORA]
alpha = 8
dropout = 0.1
r = 32
bias = none
task = CASUAL_LM

[TRAINING]
output_dir = ./results/models_pp
optim = adamw_torch
bf16 = True 

per_device_train_batch_size = 8
gradient_accumulation_steps = 2

save_steps = 10
logging_steps = 1

max_grad_norm = 0.3
warmup_ratio = 0.1

max_seq_length = 512
max_steps = 3

lr_scheduler_type = cosine
learning_rate = 2e-4

reentrant = False